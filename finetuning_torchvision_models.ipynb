{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer Learning\n",
    "# we will take a deeper look at how to finetune and feature extract the torchvision models, all of which have been pretrained on the 1000-class Imagenet dataset.\n",
    "# this will give an indepth look at how to work with several modern CNN architectures, and will build an intuition for finetuning any PyTorch model.\n",
    "# since each model architecture is different, there is no boilerplate finetuning code that will work in all scenarios. Rather, the researcher must look at the existing architecture and make custom adjustments for each model.\n",
    "\n",
    "# Two types of transfer learning: finetuning and feature extraction\n",
    "    # In finetuning, we start with a pretrained model and update all of the model’s parameters for our new task, in essence retraining the whole model.\n",
    "    #  In feature extraction, we start with a pretrained model and only update the final layer weights from which we derive predictions. It is called feature extraction because we use the pretrained CNN as a fixed feature-extractor, and only change the output layer.\n",
    "\n",
    "# In general both transfer learning methods follow the same few steps:\n",
    "    # Initialize the pretrained model\n",
    "    # Reshape the final layer(s) to have the same number of outputs as the number of classes in the new dataset\n",
    "    # Define for the optimization algorithm which parameters we want to update during training\n",
    "    # Run the training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  0.4.1\n",
      "Torchvision Version:  0.2.1\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "print(\"PyTorch Version: \", torch.__version__)\n",
    "print(\"Torchvision Version: \", torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs\n",
    "\n",
    "# we assume the format of the directory conforms to the ImageFolder structure\n",
    "data_dir = './hymenoptera_data'\n",
    "model_name = \"squeezenet\"    # [resnet, alexnet, vgg, squeezenet, densenet, inception]\n",
    "num_classes = 2\n",
    "batch_size = 8    # change depending on how much memory you have\n",
    "num_epochs = 15\n",
    "\n",
    "# If feature_extract = False, the model is finetuned and all model parameters are updated.\n",
    "# If feature_extract = True, only the last layer parameters are updated, the others remain fixed.\n",
    "feature_extract = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training and validation code\n",
    "\n",
    "# The is_inception flag is used to accomodate the Inception v3 model, as that architecture uses an auxiliary output and the overall model loss respects both the auxiliary output and the final output\n",
    "\n",
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, is_inception=False):\n",
    "    since = time.time()\n",
    "    \n",
    "    val_acc_history = []\n",
    "    \n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs-1))\n",
    "        print('-'*10)\n",
    "        \n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()    # Set model to training mode\n",
    "            else:\n",
    "                model.eval()     # Set model to evaluate mode\n",
    "                \n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            \n",
    "            # Iterate over the data\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # forward\n",
    "                # track history only if train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Get model outputs and calculate loss\n",
    "                    # Special case for inception because in training it has an auxiliary output.\n",
    "                    # In train mode we calculate the loss by summing the final output and the auxiliary output\n",
    "                    # but in testing we only consider the final output.\n",
    "                    if is_inception and phase == 'train':\n",
    "                        outputs, aux_outputs = model(inputs)\n",
    "                        loss1 = criterion(outputs, labels)\n",
    "                        loss2 = criterion(aux_outputs, labels)\n",
    "                        loss = loss1 + 0.4*loss2\n",
    "                        \n",
    "                    else:\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                        \n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        \n",
    "                # statistics\n",
    "                running_loss +=loss.item()*inputs.size(0)\n",
    "                running_corrects +=torch.sum(preds==labels.data)\n",
    "            \n",
    "            epoch_loss = running_loss/len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double()/len(dataloaders[phase].dataset)\n",
    "            \n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "            \n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                \n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "        \n",
    "        print()\n",
    "        \n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed//60, time_elapsed%60))\n",
    "    print('Best val Acc: {:.4f}'.format(best_acc))\n",
    "    \n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Model Parameters’ .requires_grad attribute\n",
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Z:\\Anaconda3\\lib\\site-packages\\torchvision\\models\\squeezenet.py:94: UserWarning: nn.init.kaiming_uniform is now deprecated in favor of nn.init.kaiming_uniform_.\n",
      "  init.kaiming_uniform(m.weight.data)\n",
      "Z:\\Anaconda3\\lib\\site-packages\\torchvision\\models\\squeezenet.py:92: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
      "  init.normal(m.weight.data, mean=0.0, std=0.01)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SqueezeNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 96, kernel_size=(7, 7), stride=(2, 2))\n",
      "    (1): ReLU(inplace)\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "    (3): Fire(\n",
      "      (squeeze): Conv2d(96, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace)\n",
      "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU(inplace)\n",
      "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU(inplace)\n",
      "    )\n",
      "    (4): Fire(\n",
      "      (squeeze): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace)\n",
      "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU(inplace)\n",
      "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU(inplace)\n",
      "    )\n",
      "    (5): Fire(\n",
      "      (squeeze): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace)\n",
      "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU(inplace)\n",
      "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU(inplace)\n",
      "    )\n",
      "    (6): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "    (7): Fire(\n",
      "      (squeeze): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace)\n",
      "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU(inplace)\n",
      "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU(inplace)\n",
      "    )\n",
      "    (8): Fire(\n",
      "      (squeeze): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace)\n",
      "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU(inplace)\n",
      "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU(inplace)\n",
      "    )\n",
      "    (9): Fire(\n",
      "      (squeeze): Conv2d(384, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace)\n",
      "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU(inplace)\n",
      "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU(inplace)\n",
      "    )\n",
      "    (10): Fire(\n",
      "      (squeeze): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace)\n",
      "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU(inplace)\n",
      "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU(inplace)\n",
      "    )\n",
      "    (11): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "    (12): Fire(\n",
      "      (squeeze): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace)\n",
      "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU(inplace)\n",
      "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU(inplace)\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.5)\n",
      "    (1): Conv2d(512, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (2): ReLU(inplace)\n",
      "    (3): AvgPool2d(kernel_size=13, stride=1, padding=0)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Initialize and Reshape the networks\n",
    "# Reshaping is not an automatic procedure and is unique to each model\n",
    "# The goal here is to reshape the last layer to have the same number of inputs as before, and to have the same number of outputs as the number of classes in the dataset.\n",
    "# notice that inception_v3 requires the input size to be (299,299), whereas all of the other models expect (224,224).\n",
    "\n",
    "# Resnet\n",
    "# There are several variants of different sizes, including Resnet18, Resnet34, Resnet50, Resnet101, and Resnet152.\n",
    "# the last layer is a fully connected layer\n",
    "\n",
    "# Alexnet\n",
    "# When we print the model architecture, we see the model output comes from the 6th layer of the classifier.\n",
    "\n",
    "# VGG\n",
    "# When we print the model architecture, we see the model output comes from the 6th layer of the classifier.\n",
    "\n",
    "# Squeezenet\n",
    "# uses a different output structure than any of the other models shown here.\n",
    "# Torchvision has two versions of Squeezenet, we use version 1.0.\n",
    "# The output comes from a 1x1 convolutional layer which is the 1st layer of the classifier\n",
    "# To modify the network, we reinitialize the Conv2d layer to have an output feature map of depth 2 as\n",
    "\n",
    "# Densenet\n",
    "# Torchvision has four variants of Densenet but here we only use Densenet-121. The output layer is a linear layer with 1024 input features\n",
    "\n",
    "# Inception_v3\n",
    "# This network is unique because it has two output layers when training.\n",
    "# The second output is known as an auxiliary output and is contained in the AuxLogits part of the network.\n",
    "# The primary output is a linear layer at the end of the network.\n",
    "# Note, when testing we only consider the primary output.\n",
    "\n",
    "def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n",
    "    model_ft = None\n",
    "    input_size = 0\n",
    "    \n",
    "    if model_name == \"resnet\":\n",
    "        model_ft = models.resnet18(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_fltrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_fltrs, num_classes)\n",
    "        input_size = 224\n",
    "    \n",
    "    elif model_name == \"alexnet\":\n",
    "        model_ft = models.alexnet(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_fltrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_fltrs, num_classes)\n",
    "        input_size = 224\n",
    "        \n",
    "    elif model_name == \"vgg\":\n",
    "        model_ft = models.vgg11_bn(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_fltrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_fltrs, num_classes)\n",
    "        input_size = 224\n",
    "        \n",
    "    elif model_name == \"squeezenet\":\n",
    "        model_ft = models.squeezenet1_0(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1, 1), stride=(1, 1))\n",
    "        model_ft.num_classes = num_classes\n",
    "        input_size = 224\n",
    "    \n",
    "    elif model_name == \"densenet\":\n",
    "        model_ft = models.densenet121(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_fltrs = model_ft.classifier.in_features\n",
    "        model_ft.classifier = nn.Linear(num_fltrs, num_classes)\n",
    "        input_size = 224\n",
    "        \n",
    "    elif model_name == \"inception\":\n",
    "        model_ft = models.inception_v3(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        # Handle the auxilary net\n",
    "        num_fltrs = model_ft.AuxLogits.fc.in_features\n",
    "        model_ft.AuxLogits.fc = nn.Linear(num_fltrs, num_classes)\n",
    "        # Handle the primary net\n",
    "        num_fltrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_fltrs, num_classes)\n",
    "        input_size = 299\n",
    "    \n",
    "    else:\n",
    "        print(\"Invalid model name, exiting...\")\n",
    "        exit()\n",
    "    \n",
    "    return model_ft, input_size\n",
    "\n",
    "# Initialize the model for this run\n",
    "model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n",
    "\n",
    "# Print the model we just initialized\n",
    "print(model_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Datasets and Dataloaders...\n"
     ]
    }
   ],
   "source": [
    "# Load Data\n",
    "# now we know what should be the input size must be, we can initialize the data transforms, image datasets, and the dataloaders.\n",
    "# notice, the models were pretrained with the hard-coded normalization values, as described here.\n",
    "\n",
    "# Data augmentation and normalization for training\n",
    "# Just normalization for validation\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(input_size),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(input_size),\n",
    "        transforms.CenterCrop(input_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "}\n",
    "\n",
    "print(\"Initializing Datasets and Dataloaders...\")\n",
    "\n",
    "# Create training and validation datasets\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'val']}\n",
    "\n",
    "# Create training and validation dataloaders\n",
    "dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=4) for x in ['train', 'val']}\n",
    "\n",
    "# Detect if we have a GPU available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters to learn: \n",
      "\t classifier.1.weight\n",
      "\t classifier.1.bias\n"
     ]
    }
   ],
   "source": [
    "# Create the Optimizer\n",
    "\n",
    "# send the model to GPU\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "# Gather the parameters to be optimized/updated in this run.\n",
    "params_to_update = model_ft.parameters()\n",
    "print(\"Parameters to learn: \")\n",
    "if feature_extract:\n",
    "    params_to_update = []\n",
    "    for name, param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            params_to_update.append(param)\n",
    "            print(\"\\t\", name)\n",
    "else:\n",
    "    for name, param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            print(\"\\t\", name)\n",
    "    \n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.SGD(params_to_update, lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/14\n",
      "----------\n",
      "train Loss: 0.7312 Acc: 0.6516\n",
      "val Loss: 0.5726 Acc: 0.7255\n",
      "\n",
      "Epoch 1/14\n",
      "----------\n",
      "train Loss: 0.4201 Acc: 0.8197\n",
      "val Loss: 0.5286 Acc: 0.7778\n",
      "\n",
      "Epoch 2/14\n",
      "----------\n",
      "train Loss: 0.3345 Acc: 0.8525\n",
      "val Loss: 0.4564 Acc: 0.8170\n",
      "\n",
      "Epoch 3/14\n",
      "----------\n",
      "train Loss: 0.3419 Acc: 0.8525\n",
      "val Loss: 0.3931 Acc: 0.8497\n",
      "\n",
      "Epoch 4/14\n",
      "----------\n",
      "train Loss: 0.2877 Acc: 0.8689\n",
      "val Loss: 0.3521 Acc: 0.8693\n",
      "\n",
      "Epoch 5/14\n",
      "----------\n",
      "train Loss: 0.2684 Acc: 0.8975\n",
      "val Loss: 0.3714 Acc: 0.8889\n",
      "\n",
      "Epoch 6/14\n",
      "----------\n",
      "train Loss: 0.2444 Acc: 0.9180\n",
      "val Loss: 0.3828 Acc: 0.8954\n",
      "\n",
      "Epoch 7/14\n",
      "----------\n",
      "train Loss: 0.2196 Acc: 0.9139\n",
      "val Loss: 0.3511 Acc: 0.8954\n",
      "\n",
      "Epoch 8/14\n",
      "----------\n",
      "train Loss: 0.2392 Acc: 0.9016\n",
      "val Loss: 0.3598 Acc: 0.9020\n",
      "\n",
      "Epoch 9/14\n",
      "----------\n",
      "train Loss: 0.2533 Acc: 0.8770\n",
      "val Loss: 0.3703 Acc: 0.8824\n",
      "\n",
      "Epoch 10/14\n",
      "----------\n",
      "train Loss: 0.1958 Acc: 0.8934\n",
      "val Loss: 0.3985 Acc: 0.8954\n",
      "\n",
      "Epoch 11/14\n",
      "----------\n",
      "train Loss: 0.2108 Acc: 0.9098\n",
      "val Loss: 0.3652 Acc: 0.9085\n",
      "\n",
      "Epoch 12/14\n",
      "----------\n",
      "train Loss: 0.2042 Acc: 0.8975\n",
      "val Loss: 0.3902 Acc: 0.8627\n",
      "\n",
      "Epoch 13/14\n",
      "----------\n",
      "train Loss: 0.1794 Acc: 0.9262\n",
      "val Loss: 0.3730 Acc: 0.8824\n",
      "\n",
      "Epoch 14/14\n",
      "----------\n",
      "train Loss: 0.2627 Acc: 0.9098\n",
      "val Loss: 0.3905 Acc: 0.8889\n",
      "\n",
      "Training complete in 1m 45s\n",
      "Best val Acc: 0.9085\n"
     ]
    }
   ],
   "source": [
    "# Running training and validation step\n",
    "\n",
    "# setup loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train and Evaluate\n",
    "model_ft, hist = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft, num_epochs, is_inception=(model_name==\"inception\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Z:\\Anaconda3\\lib\\site-packages\\torchvision\\models\\squeezenet.py:94: UserWarning: nn.init.kaiming_uniform is now deprecated in favor of nn.init.kaiming_uniform_.\n",
      "  init.kaiming_uniform(m.weight.data)\n",
      "Z:\\Anaconda3\\lib\\site-packages\\torchvision\\models\\squeezenet.py:92: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
      "  init.normal(m.weight.data, mean=0.0, std=0.01)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/14\n",
      "----------\n",
      "train Loss: 0.7077 Acc: 0.5123\n",
      "val Loss: 0.6931 Acc: 0.4575\n",
      "\n",
      "Epoch 1/14\n",
      "----------\n",
      "train Loss: 0.6927 Acc: 0.5369\n",
      "val Loss: 0.6931 Acc: 0.4575\n",
      "\n",
      "Epoch 2/14\n",
      "----------\n",
      "train Loss: 0.6924 Acc: 0.4959\n",
      "val Loss: 0.6931 Acc: 0.4575\n",
      "\n",
      "Epoch 3/14\n",
      "----------\n",
      "train Loss: 0.6911 Acc: 0.4959\n",
      "val Loss: 0.6927 Acc: 0.5948\n",
      "\n",
      "Epoch 4/14\n",
      "----------\n",
      "train Loss: 0.6988 Acc: 0.4467\n",
      "val Loss: 0.6931 Acc: 0.4575\n",
      "\n",
      "Epoch 5/14\n",
      "----------\n",
      "train Loss: 0.6932 Acc: 0.5041\n",
      "val Loss: 0.6931 Acc: 0.4575\n",
      "\n",
      "Epoch 6/14\n",
      "----------\n",
      "train Loss: 0.6932 Acc: 0.5041\n",
      "val Loss: 0.6931 Acc: 0.4575\n",
      "\n",
      "Epoch 7/14\n",
      "----------\n",
      "train Loss: 0.6931 Acc: 0.5041\n",
      "val Loss: 0.6931 Acc: 0.4575\n",
      "\n",
      "Epoch 8/14\n",
      "----------\n",
      "train Loss: 0.6932 Acc: 0.5041\n",
      "val Loss: 0.6931 Acc: 0.4575\n",
      "\n",
      "Epoch 9/14\n",
      "----------\n",
      "train Loss: 0.6932 Acc: 0.5041\n",
      "val Loss: 0.6931 Acc: 0.4575\n",
      "\n",
      "Epoch 10/14\n",
      "----------\n",
      "train Loss: 0.6931 Acc: 0.5041\n",
      "val Loss: 0.6931 Acc: 0.4575\n",
      "\n",
      "Epoch 11/14\n",
      "----------\n",
      "train Loss: 0.6931 Acc: 0.5082\n",
      "val Loss: 0.6931 Acc: 0.4575\n",
      "\n",
      "Epoch 12/14\n",
      "----------\n",
      "train Loss: 0.6932 Acc: 0.5041\n",
      "val Loss: 0.6931 Acc: 0.4575\n",
      "\n",
      "Epoch 13/14\n",
      "----------\n",
      "train Loss: 0.6932 Acc: 0.5041\n",
      "val Loss: 0.6931 Acc: 0.4575\n",
      "\n",
      "Epoch 14/14\n",
      "----------\n",
      "train Loss: 0.6932 Acc: 0.5041\n",
      "val Loss: 0.6931 Acc: 0.4575\n",
      "\n",
      "Training complete in 2m 22s\n",
      "Best val Acc: 0.5948\n"
     ]
    }
   ],
   "source": [
    "# comparison with model trained from scratch\n",
    "# lets see how the model learns if we do not use transfer learning.\n",
    "\n",
    "# Initialize the non-pretrained version of the model used for this run\n",
    "scratch_model, _ = initialize_model(model_name, num_classes, feature_extract=False, use_pretrained=False)\n",
    "scratch_model = scratch_model.to(device)\n",
    "scratch_optimizer = optim.SGD(scratch_model.parameters(), lr=0.001, momentum=0.9)\n",
    "scratch_criterion = nn.CrossEntropyLoss()\n",
    "_, scratch_hist = train_model(scratch_model, dataloaders_dict, scratch_criterion, scratch_optimizer, num_epochs, is_inception=(model_name==\"inception\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3Xl8FeXZ//HPNyGQBAhbUFbZRFkCokbFtahgxQVstbU+tlZqpdXHrVZbW39atdrHunR5fKxK3de6W7RQVyzusogQQAQRIYR9CQESSMj1+2Mmh0PIcpKck5OE6/165ZUzZ2buuc6cOXPNfc/MPTIznHPOOYCUZAfgnHOu6fCk4JxzLsKTgnPOuQhPCs455yI8KTjnnIvwpOCccy7Ck0IMJPWVZJJahcNTJf04lmnrsazfSnqwIfG65q+h21Ecln+spMWStko6K8HLSg2Xc0A8p20OJD0p6aZkxxFtn0gKkl6XdEsV74+XtLquPzwzG2tmj8UhrlGS8iuV/Qcz+2lDy65lmSbpV4laRksk6cJwvV1b6f18SaOSFFYi3QL8n5m1M7NXokeEO+WKv3JJxVHD59d1QWa2K1zO8nhOW1eSbpVUWunzrY/3cpq6fSIpAI8CP5KkSu//CHjKzMoaP6Sk+TGwMfzfqJJ11BtHG4FfS8pKdiB1Uc/13geYX9WIcKfczszaAcuBM6PeeypOy0+Wp6I/n5llJzugxravJIVXgM7A8RVvSOoEnAE8Hg6fLukzSVskraipSifpXUk/DV+nSrpL0npJS4HTK007QdJCSUWSlkr6Wfh+W2Aq0CPqqKSHpJskPRk1/zhJ8yVtDpc7OGrcMknXSJorqVDSs5LSa4g7EzgH+G9goKTcSuOPk/RhuKwVki4M38+QdLekb8LlvB++t1dNJ4xpdPj6JkkvhFXkLcCFko6U9FG4jFWS/k9S66j5h0p6U9JGSWvC5rRukrZL6hI13eGS1klKq7T8HuGRa+eo9w4Nv580SQdK+k/4OdZLera69VWFhcBHwC+qWb+PSro1aniP9ROum2vD72ubpIck7a+gObJI0lvhdhntJ5IKwnX1y6iyUiRdJ+krSRskPVfxmbW76ekiScuBd6qJ92JJS8J1PVlSj/D9r4D+wKvhdtmmDuuo4oj7WUnPSCoCfijpaEkfR33v/1vx3UlqFcbbNxx+MhxfsV4+ktSvrtOG48dK+jL8vu+R9EHFdl3Hz1Sx3MslfR1uO7dLSgnHp0i6MfyNrA23hayo+U8IP3+hgt/Wj6KK71zNZ00JP9vacL65kobUNfY6M7N94g/4O/Bg1PDPgDlRw6OAYQSJcjiwBjgrHNcXMKBVOPwu8NPw9c+BL4DeBIlnWqVpTwcGAAK+BWwHDotaZn6lOG8CngxfHwRsA8YAacCvgCVA63D8MuBToEe47IXAz2tYBz8CVgGpwKvA/0aNOwAoAs4Ll9UFGBGOuzf8zD3DeY8B2lQT/zJgdNRnKQXOCtdrBnA4MBJoFa7XhcBV4fTtw/h+CaSHw0eF46YAl0Qt58/APdV8zneAi6OG7wTuD18/A1wfxpMOHBfj9nMh8D4wAtgMdA7fzwdGha8fBW6ttE3lV1o3HwP7h+tyLTAbODRcn+8Av6u0zT0DtCXYNtdFrdurwrJ6hfM+ADxTad7Hw3kzqvg8JwHrgcPC+e8Bplf1PdayXvaaDrgV2AmcGfW9HwEcFX7v/YEvgcvC6VuF8fYNh58MY8sl2BafZfdvoi7T7kewTY8Px11NsD1eWM1nuRV4tJpxFct9C+gUruMlFWUBE8PP1I9gu/0n8Eg4rl8Yx/fDcrLZ/duqKf7TCX7fHcL1OATolvB9ZaIX0FT+gOOAwoofCPAB8Isapv8L8OdKP7KqksI7RO2IgVOip62i3FeAK8PXo6g5KdwAPBc1LgVYye6d0DLgh1Hj7yDc+VWz7LeAv4SvzyPYyaSFw78BXq5inhSgGDikinFVxb+MPZPC9OriCae5qmK5YUyfVTPducAH4etUYDVwZDXT/hR4J3wtYAVwQjj8ODAJ6FXH7edC4P3w9XPAH8PXdU0K50cNvwjcFzV8OfBKpW1uUKXv96Hw9ULg5Khx3Ql2eK2i5u1fw+d5CLgjarhdOH/fyt9jLetlr+kIdq7v1DLfNcDz4euqdvT3R007Dsirx7Q/Ad6LGieCg44Lq4mpIpltjvp7s9JyR0dNfwXwevj6P8DEqHFDgR0Ev58bKj5rFcusKf5TCA44jwJS6rK9NuRvX2k+wszeJ9gJjpfUn+DI5emK8ZKOkjQtbJIoJKgBxNKe2INgp1Phm+iRYfX147CKvhk4LcZyK8qOlGdm5eGyekZNszrq9XaCH/deJPUGTgQq2nz/SXCkXNHc1Rv4qopZs8PpqhoXi+h1g6SDJL2m4AT/FuAP7F4f1cVQEe+Q8LsbAxSa2afVTPsCcHTYHHICwY/5vXDcrwh2Dp8qaJb7ST0+043AJZK61WPeNVGvi6sYrvz9Vd62eoSv+wAvh80xmwmSxC6CWkhV81ZWedvaCmxgz22rISp/74Mk/Svqe7+Fmn8HMW3XtUy7x2/Tgj3tHs2dVXjazDpG/Y2pNL6672OP9Rm+bg10pebtutr4zewN4H7gPmCNpPslta8l/gbbZ5JC6HHgAoJmlDfMLPoH+TQwGehtZh0IvozKJ6arsorgS68QuVQubIt9EbgL2N/MOhI0g1SUa7WUXUDw468oT+GyVsYQV2U/Ivi+X5W0GlhKsLO/IBy/gqCZq7L1QEk147YBmVHxpRL8CKJV/oz3ERz9DDSzLOC37F4f1cWAmZUQHKGfH36WJ6qaLpx2M/AGQXX9vwiaVSwct9rMLjazHgRNiH+TdGB1ZVVT/hfAS2Hs0fZYH0B9kkZllbetgvD1CmBspR1YuplFbxs1bV+Vt622BE2G9dm2qlJ52Q8AecCB4fd+I7H9vhpiFUHzGhD5/TQ06VX3feyxPsNxOwkORKvdrmtjZn8xs8OAHILmo6vrU05d7ItJYTRwMVD5ktL2wEYzK5F0JMHOJBbPAVdI6hWeJLwualxrgvbadUCZpLEEVcIKa4AukjrUUPbpkk4OT8r9kqBK+mGMsUW7ALiZoE284u/ssPwuBDWI0ZK+H55U6yJpRFg7eRj4k4KTuKnhScM2BG2o6QpO0qcB/y/8vDVpD2wBtkoaBFwSNe41oJukqyS1kdRe0lFR4x8naMYZR1DtrsnT4Wc+mz1rhN+TVLGj2ESw89pVS1lVuRmYAHSMem8OcJqkzmEt4qp6lFvZDZIyJQ0Nl1dxYvx+4DZJfQAkdZU0vg7lPg1MkDQi/C7/AHxiZsviEHNV2hM0325TcLHEzxK0nGivAYdJOlPBFVBXsvdBS139SlJHBfdJXMHu7+MZ4GoFJ/nbA7cRHIyUE2yrp0o6O/xtZUs6pLYFKbgo48gw9m0ESaY+22qd7FNJIdzgPyQ4+Ta50uhLgVsUXC1xI8EOORZ/B14HPic4afhS1PKKCDac5wh2QP8VvdzwiPMZYGnYDNAjqlzMbBHwQ4KTgOsJTtydaWY7Y4wNAEkjCdqZ7w2PlCv+JhOcLDvPguu+TyNIPBsJdnAVG+41wDxgRjjujwRtnIUE6+1BgiPMbdRePb8mXA9FBOsucvVPuL7GhJ9zNbCYoMmrYvwHQDkwO4ad12RgILDGzD6Pev8I4BNJW8NprjSzr8P1NF8xXmcfzvMEwbZU4QmC7WAZQU2lLlc2Vec/BN/R28BdYZMCwF/D+N8It9mPCdqeY2JmbxO0db9IcEQ9APhBHOKtzi8JLoMuIqg1xGPd1ChsCTgX+BNB09gA4DOCA6vqnK8971PYqqir3ggu0JgTlvMywXkk2L0tv0dQCy8iSEIV28qZwK8Jfj+zCS4cqE1HgnM/mwm2qVUEF1gklMJatXPNgqR3CNp9/a5vVydh82YBcI6ZvVfb9JXmbUVwIr5fAmtTTcI+VVNwzZukIwguoUz4UaZrGSSdKqlD2ER2A1BGcJmnq0bCkoKkh8ObLvKqGa/wxowl4U0ZhyUqFtf8SXqM4JLaq8JmJudicRxBc8564FSCe49qaj7a5yWs+UjSCcBW4HEzy6li/GkE12WfRtAW+lczi7lN1DnnXPwlrKZgZtMJTqpUZzxBwjAz+xjoKKl7ouJxzjlXu2R2VNWTPW8EyQ/fW1V5QkkTCW4jp23btocPGjSoUQJ0zrmWYtasWevNrNZLcpOZFKq6caXKtiwzm0TQNQG5ubk2c+bMRMblnHMtjqRvap8quVcf5bPn3YG92H13oHPOuSRIZlKYDFwQXoU0kqAvm72ajpxzzjWehDUfSXqGoJfIbAV9yv+OoGtYzOx+gj6ATiO4W3M7wS38zjnnkihhScHMzqtlvBE87MU551wT4Xc0O+eci/Ck4JxzLsKTgnPOuQhPCs455yI8KTjnnIvwpOCccy7Ck4JzzrkITwrOOeciPCk455yL8KTgnHMuwpOCc865CE8KzjnnIjwpOOeci0jmk9ecc65Ku8qNVYXFrNhYzIqN29lSUsrpw7vTvUNGskNr8TwpONeElZTuAiA9LTXJkcSXmbF5eykrNm1n+cbtrNhYzPKN28kPhws2F1O6a8+n894+9QvGjejBxBP6M6hbVpIib/k8KTiXQGbGtp27KCwuZUtxKYVRf1ui3ttSUrbXuMLiUnaUlSPBgK7tyOmRRU7PDgzt0YGhPbPISk9L9serUUnpLvI3BTv8FZu2s3zD9jAJFJO/cTtFO8r2mL5z29b07pTBsJ4dOG1Ydw7onEnvTpkc0DmTcjMe/XAZz85YwUuzVzLq4K5MPKE/R/fvglTV495dfSl41k3zkZubazNnzkx2GK4Jit4BF26P2sGW7LlD3lJcyq4EbPbl5bb3skrK2FVe/cIkaN+mFR0y08hKT6NDxu6/rPD/jrJyFhQUkrdyC6u3lETm7dslk6E9O5DTowM5PbPI6dGBTm1bx/+DVWNXubFmSwkrNoZH+5uCpp6K4bVFO/aYvk2rFHp3zgx39hn07py5e7hzJu3a1H6MumnbTp78+Bse+2gZ67fuZFjPDkw8oT9jc7rRKtVPkdZE0iwzy611Ok8KrikqLzeWbdhGweaSPY+gS6o+0o51B5yVnkZWRivSUuK/A5GgfXr0Dr1V8LqanX1WRhrt27QiJSX2I911RTuYX1DI/IIt5K0sZN7KQvI3FUfG9+yYEUkQOT2DGsV+7dPr/ZkKt5eGO/yKZp7dO/+Vm4rZuat8j8/fPSu90s4+I3LE37V9m7gd1ZeU7uKl2St58L2lLF2/jd6dM7jo2H58/4jeZLZuOQ0gO8r2rGX27pTJfln1+z49Kbhmo2xXOUvXbyNvZXA0nLeykPkFhWzbuWuvadNSFdmh7r2zbbXncPruHXCHzDTata7bDri52Lx9ZyRJ5BVsYf7KQpau3xYZv1/7Ngzr2SGsVQRNUN07pCOJktJdrNwctueHO/zdzTzbKSrZs4mnY2ZapEmnV9QO/4DOmfTomEHrVo17tF5ebry5cA2Tpi9l1jeb6JiZxgUj+3DBMX3JbtemUWOpSkXttaqmw8iBTTVNhxXNh9FuPSuHH47sU69YPCm4JmlnWTmL1xYxf+UW5q0sJK+gkIWrtlBSGmz86WkpDOmeFdmJ9e3Sdo+dfkZaqrchx6CopJQFBVsiSSKvoJAla7dSUZHq0rY1aakpezRHAbRulRJp2qnY4fcOj/h7d85s0ucxZi7byAPTl/LWwjW0Tk3h7MN7cfHx/emX3Tahyy0vN77eUHFQs7uZr2IHXxZD82FWRtpeBzQdMtOiDoCCA55B3bLo1sFrCnvwpNB8lJTuYtHqIvIKdv9YFq0uijQ5tGvTiqHhkWtFk0f/ru1IbYFH801B8c5dLFi1hfnh97GrnD2beDpn0rVdm2Zfm/pq3VYefG8pL85eSemuck4Zsj8TTxjA4X06Nbjssl3lfLVuW3BAE9ZoFxRsidRqW7dKYXD3LHp3yqiyyXCPnX5GGu3SWzXa9u5JwTWq7TvLWLhqC/PygyaMvJWFLF67NdLG3yEjLdjxR06MdqBP58xmvwNyTdfaohIe+3AZT368nMLiUnL7dOJn3xrAyYP2i2m721lWzpdrisJmueCgZuGqLZEmnYy01MhBTcX/A/drR1oTPeHtScElzJaKpomK6nLBFpau27NpIqdnB4aFNYChPTrQq1OGN/u4pNi2o4xnZ6zgofe/ZuXmYgZ0bcvFx/fnrEN7Ru7/KCndxcJVeza3LVpdFLlXon2bVgyNOoGf0zOLftnNq1brScHFxaZtwUnMivb/+SsLWbZhe2R8t6z0yI4/SAId2D8rfleZOBcvZbvK+de8VUyavpT5BVvIbteGYwZ04cs1RXvUajtmpgXntKIu9T2gBdRqPSm4OltXtCOy458XngNYuXn35Y69OmWQ06MDw3oF1eWhPTrQtX3yr/Bwri7MjA+WbGDSe0v5cnURg7u3j9wUmNMzi54dW2atNtak0HIu6HUxMzNWbynZ4/LPeSsLWbNl981G/bLbcugBHbng6D6RNtOOmY13Y5RziSKJ4wZmc9zA7GSH0iR5UthHrC0q4fX5a3hn4RrmrSxk/dadAKSEXSgcOyA7ch37kB5ZtG/Clx465xLHk0ILVrC5mH/nrebfeauZ8c1GzKB/dltOPHi/sAmoA4O7t29Rd4A65xrG9wYtzIqN25mat4qpeav5bPlmAAZ1a89VJx/EacO6MXD/9kmO0DnXlHlSaAGWrtvK1LBGMG9lIQA5PbO49tsHMzanG/27tktyhM655sKTQjO1eE0RU+atZmreKr5YXQTAiN4d+e1pgxib053enTOTHKFzrjnypNBMmBkLVxUxNW8VU+at4qt125Agt08nbjxjCKfmdKNHR38qlXOuYTwpNGFmxtz8QqbmBTWCbzZsJ0VwVL8uXHhMX749tFu9u9F1zrmqeFJogpasLeK5mfn8a+4qVm4uplWKOHpAF37+rQGMGbJ/k+gS2DnXMnlSaCK27yzjX3NX8eyMFcz8ZhOtUsTxA7O5avRAxgzZ328cc841ioQmBUmnAn8FUoEHzez2SuMPAB4DOobTXGdmUxIZU1OTt7KQZz5dzuQ5BRTtKKN/dlt+M3YQ3z2sl3ch4ZxrdAlLCpJSgXuBMUA+MEPSZDNbEDXZ/wOeM7P7JA0BpgB9ExVTU1FYXMrkOSv5x4wVzC/YQptWKZw+rDvnHtGbI/t1bpH9rjjnmodE1hSOBJaY2VIASf8AxgPRScGArPB1B6AggfEklZkx85tNPPPpcqbMW0VJaTmDu2dxy/ihjB/Rkw4Z3q2Ecy75EpkUegIroobzgaMqTXMT8Iaky4G2wOiqCpI0EZgIcMABB8Q90ETasHUHL87O5x8zVrB03TbatWnFdw/rxQ+O6M2wnh28VuCca1ISmRSq2ttV7qf7POBRM7tb0tHAE5JyzGyPp1Wb2SRgEgRdZyck2jgqLzfeX7Kef8xYzpsL1lC6yzi8TyfuOGcApw/rTts2fn7fOdc0JXLvlA/0jhruxd7NQxcBpwKY2UeS0oFsYG0C40qYVYXFPDcjn+dmrmDl5mI6ZaZxwdF9OfeI3hzkfQ4555qBRCaFGcBASf2AlcAPgP+qNM1y4GTgUUmDgXRgXQJjirvSXeW888Va/vHpcv7z5TrKDY47MJvrxg7ilKH706ZVarJDdM65mCUsKZhZmaTLgNcJLjd92MzmS7oFmGlmk4FfAn+X9AuCpqULrZk8Cm5XufHKZyv5y9tfsmJjMftnteHSUQfy/dzeHNDF+x1yzjVP/jjOOjIzXp+/mrvf+JLFa7eS0zOLy08ayMmD9qNVakrS4nLOuZr44zjjzMyYvng9d72+iHkrCxnQtS33nX8Yp+Z08yuInHMthieFGMxctpE7Xl/Ep19vpGfHDO763iF859CepKZ4MnDOtSyeFGqQt7KQu99YxLRF68hu14Zbxg/l3CN6+8lj51yL5UmhCl+t28qf3vySf81dRYeMNH596iB+fEwff5axc67F871clPxN2/nrW4t5cXY+6WmpXHHSgVx0fH/vgsI5t8/wpACsLSrhb9O+4ulPloNgwrH9uGTUAH9ugXNun7NPJ4XC7aU8MP0rHvlgGTt3lfP93F5cftJAf6ylc26ftU8mhW07ynj0w2Xc/5+vKCopY9whPfjFmIPol9022aE551xS7VNJoaR0F09/spy/vbuE9Vt3MnrwfvzylIMZ3D2r9pmdc24fsM8khdfnr+bmyfMpKCxhZP/OPPCjQRzep1Oyw3LOuSZln0kKZkbXrHTuOOcQjj2wi9+F7JxzVag1KUhKNbNdjRFMIn17aDe+PdS7pHDOuZrE0oPbEkl3hs9QbrYkeUJwzrlaxJIUhgNfAg9K+ljSREl+ZtY551qgWpOCmRWZ2d/N7BjgV8DvgFWSHpN0YMIjdM4512hqTQqSUiWNk/Qy8FfgbqA/8CowJcHxOeeca0SxXH20GJgG3GlmH0a9/4KkExITlnPOuWSIJSkMN7OtVY0wsyviHI9zzrkkiuVE872SOlYMSOok6eEExuSccy5JYrr6yMw2VwyY2Sbg0MSF5JxzLlliSQopkiL9QUjqzD50J7Rzzu1LYtm53w18KOmFcPh7wG2JC8k551yy1JoUzOxxSbOAEwEB3zWzBQmPzDnnXKOLqRnIzOZLWgekA0g6wMyWJzQy55xzjS6Wm9fGSVoMfA38B1gGTE1wXM4555IglhPNvwdGAl+aWT/gZOCDhEblnHMuKWJJCqVmtoHgKqQUM5sGjEhwXM4555IglnMKmyW1A6YDT0laC5QlNiznnHPJEEtNYTywHfgF8G/gK+DMRAblnHMuOWqsKUhKBf5pZqOBcuCxRonKOedcUtRYUwgfw7ldUodGisc551wSxXJOoQSYJ+lNYFvFm95DqnPOtTyxJIV/hX/OOedauFi6ufDzCM45t4+I5Y7mryUtrfwXS+GSTpW0SNISSddVM833JS2QNF/S03X9AM455+Inluaj3KjX6QS9pHaubabwyqV7gTFAPjBD0uTozvQkDQR+AxxrZpsk7VeX4J1zzsVXrTUFM9sQ9bfSzP4CnBRD2UcCS8xsqZntBP5BcM9DtIuBe8MH92Bma+sYv3POuTiqtaYg6bCowRSCmkP7GMruCayIGs4Hjqo0zUHhMj4AUoGbzOzfVcQwEZgIcMABB8SwaFclM9iwBLocCFKyo3HONUGxPmSnQhlBb6nfj2G+qvY6VsXyBwKjgF7Ae5Jyoh//CWBmk4BJALm5uZXLcLH6+D54/Tcw+mY47qpkR+Oca4JiufroxHqWnQ/0jhruBRRUMc3HZlYKfC1pEUGSmFHPZbrqFMyBN2+EtLbwzu+h73HQK7f2+Zxz+5RYrj76g6SOUcOdJN0aQ9kzgIGS+klqDfwAmFxpmlcInuiGpGyC5qSYrmxydbBjK7zwE2jbFS55H9r3CIZLCpMdmXOuiYmlQ7yx0c054Unh02qbyczKgMuA14GFwHPhE9xukTQunOx1YIOkBcA04Nqwm24XT1OuhY1L4buToHN/OPtBKMyH164OzjM451wolnMKqZLamNkOAEkZQJtYCjezKcCUSu/dGPXagKvDP5cIc5+Dz5+GE34F/Y4P3jvgKDjxN/DOrTDgJDj0/OTG6JxrMmKpKTwJvC3pIkk/Ad7Ee0ttHjYuDWoDvUfCt36957jjroa+x8OUa2D94uTE55xrcmK5T+EO4FZgMDAU+H34nmvKynbCCxdBSgqc/XdIrVQpTEkNmpNapcMLE6BsR3LidM41KbGcaO4HvGtm15jZL4HpkvomOjDXQO/8Hgpmw7h7oGM193Zk9YCz/gar58Gbv2vc+JxzTVIszUfPEzxgp8Ku8D3XVC15Gz78Xzh8AgypfBN5JQePhaN+Dp/cB4v2um/QObePiSUptAq7qQAgfN06cSG5Btm6Fl7+OXQdDN/+Q2zzjL4Z9h8G/7wUtqxKbHzOuSYtlqSwLuoSUiSNB9YnLiRXb+XlQULYsQXOeRhaZ8Y2X1p6MH1pMbx0MZTvSmyczrkmK5ak8HPgt5KWS1oB/Br4WWLDcvXy0f/BV2/Dt2+D/YfUbd6uB8HYO2DZe/D+nxMTn3OuyYulm4uvgJGS2gEysyJJ+yc+NFcnK2fB2zfDoDMg96L6lXHoD2HpNJj2B+h3AvQ+Mr4xOueavFhqChVSge9JeguYnaB4XH2UbAkuP23XLbjaqL49oEpwxp+hQ6+gvOLNtc/jnGtRakwKkjIknSvpn0Ae8CeCexZ61zSfa2RTroHN3wT3I2TW+vyjmqV3CM4vFBXAq1d6NxjO7WOqTQqSngK+BE4B/g/oC2wys3fNrLy6+Vwjm/MMzH02uGO5zzHxKbNXLpx4PSx4BWY/Hp8ynXPNQk01hRxgE0Fndl+Y2S72fh6CS6b1S+Bfv4Q+x8IJ18a37GOvgv6jYOqvYd2i+JbtnGuyqk0KZnYIwcN0soC3JL0HtJfUrbGCczUo2wkv/gRatYbv/j3otiKeUlLgOw8El7W+8BMoLYlv+c65JqnGcwpm9oWZ3WhmBwO/AB4HPpX0YaNE56r39s2w6nMYfy906JmYZbTvBmfdD2vy4M0bErMM51yTEvPVR2Y2M+z7qA/wm8SF5Gq1+M3gnoQjLoZBpyd2WQedAiP/Gz6dBF9MqX1651yzVpdLUoHgGQhm9p9EBONiULQ6uGt5v6Fwyu8bZ5mjfwfdhgfdYBSubJxlOueSos5JwSVReTm8/DPYuS24bDQto3GW26oNnPNIcB7jpYneDYZzLZgnhebkw7/C0ndh7O2w36DGXXb2gXD6XfDN+/De3Y27bOdco6m1mwtJbYCzCe5TiExvZrckLiy3l/yZweMzh5wFh/04OTEcch58NQ3e/Z+gG4wDRiYnDudcwsRSU/gnMB4oA7ZF/bnGUlIYXBbavgec+df6d2PRUBKcfjd07AMv/hSKNyUnDudcwtRaUwB6mdmpCY/EVc0MXvsFFObDhKmQ0TG58aRnwTkPwUOnwOQr4PuPJy9JOefiLpaawoeShiU8Ele1OU9B3otw4m/ggKOSHU3TcbJKAAAYAUlEQVSg5+Fw8o2wcDLMeiTZ0Tjn4iiWpHAcMEvSIklzJc2TNDfRgTlg/WKYci30PR6OuzrZ0ezp6MthwEnw79/A2oXJjsY5FyexNB+NTXgUbm9lO+CFCdAqHb47Kf7dWDRUSkpwt/P9x8LzE2DitMa7RNY5lzCxPGTnG0mHAMeHb71nZp8nNqwEKPgMln+c7Chit/xjWD0PznsWsnokO5qqtd8fvnM/PHl2cOK573HJjsi5lq3v8dAtJ6GLiOWS1CuBi4GXwreelDTJzO5JaGTx9vV0ePPGZEdRN8deCQc38XP8B44Oemidfid88Vqyo3GuZTv9TwlPCrJaHqISnj842sy2hcNtgY/MbHhCI6tGbm6uzZw5s+4zlpZAWXH8A0oUpQQPvGkuSraA+Z3OziVUWmbQw0A9SJplZrm1TRfLOQUB0b/2XeF7zUtaevDnEiM9K9kROOfiIJak8AjwiaSXw+GzgIcSF5JzzrlkieVE858kvUtwaaqACWb2WaIDc8451/iqTQqSssxsi6TOwLLwr2JcZzPbmPjwnHPONaaaagpPA2cAs9jz2cwKh/snMC7nnHNJUG1SMLMzwv/9Gi8c55xzyVRrNxeS3o7lPeecc81fTecU0oFMIFtSJ3ZfhpoFNNFbbJ1zzjVETTWFnxGcTxgU/q/4+ydwbyyFSzo17EhviaTrapjuHEkmqdYbK5xzziVOTecU/gr8VdLl9enSQlIqQfIYA+QDMyRNNrMFlaZrD1wBfFLXZTjnnIuvWO5TuEdSDjAESI96//FaZj0SWGJmSwEk/YPgCW4LKk33e+AO4Jo6xO2ccy4BYjnR/DvgnvDvRIId+LgYyu4JrIgazg/fiy77UKC3mdXYk5qkiZJmSpq5bt26GBbtnHOuPmJ5yM45wMnAajObABwCxNIjU1X9I0Xud5CUAvwZ+GVtBZnZJDPLNbPcrl27xrBo55xz9RFLUig2s3KgTFIWsJbYblzLB3pHDfcCCqKG2wM5wLuSlgEjgcl+stk555Inlg7xZkrqCPyd4OqjrcCnMcw3AxgoqR+wEvgB8F8VI82sEMiuGA77V7rGzOrRL7Zzzrl4iOVE86Xhy/sl/RvIMrNan9FsZmWSLgNeB1KBh81svqRbgJlmNrkhgTvnnIu/mm5eO6ymcWY2u7bCzWwKMKXSe1U+/szMRtVWnnPOucSqqaZwd/g/HcgFPic4eTyc4J4CfyCvc861MNWeaDazE83sROAb4LDw6p/DgUOBJY0VoHPOucYTy9VHg8xsXsWAmeUBIxIXknPOuWSJ5eqjhZIeBJ4kuM/gh8DChEblnHMuKWJJChOAS4Arw+HpwH0Ji8g551zSxHJJagnBncd/Tnw4zjnnkqmmS1KfM7PvS5rHno/jBMDMhic0Muecc42upppCRXPRGY0RiHPOueSr6XkKq8L/3zReOM4555KppuajIqpoNiK4gc3MLCthUTnnnEuKmmoK7RszEOecc8kXyyWpAEjajz2fvLY8IRE555xLmlievDZO0mLga+A/wDJgaoLjcs45lwSxdHPxe4IH4HxpZv0InsL2QUKjcs45lxSxJIVSM9sApEhKMbNpeN9HzjnXIsVyTmGzpHYE3Vs8JWktUJbYsJxzziVDLDWF8UAx8Avg38BXwJmJDMo551xy1HSfwv8BT5vZh1FvP5b4kJxzziVLTTWFxcDdkpZJ+qMkP4/gnHMtXE1PXvurmR0NfAvYCDwiaaGkGyUd1GgROuecazS1nlMws2/M7I9mdijwX8B38IfsOOdcixTLzWtpks6U9BTBTWtfAmcnPDLnnHONrqYTzWOA84DTgU+BfwATzWxbI8XmnHOukdV0n8JvgaeBa8xsYyPF45xzLolq6iX1xMYMxDnnXPLFcvOac865fYQnBeeccxGeFJxzzkV4UnDOORfhScE551yEJwXnnHMRnhScc85FeFJwzjkX4UnBOedcREKTgqRTJS2StETSdVWMv1rSAklzJb0tqU8i43HOOVezhCUFSanAvcBYYAhwnqQhlSb7DMg1s+HAC8AdiYrHOedc7RJZUzgSWGJmS81sJ0Evq+OjJzCzaWa2PRz8GOiVwHicc87VIpFJoSewImo4P3yvOhcRPK9hL5ImSpopaea6deviGKJzzrloiUwKquI9q3JC6YdALnBnVePNbJKZ5ZpZbteuXeMYonPOuWg1PU+hofKB3lHDvYCCyhNJGg1cD3zLzHYkMB7nnHO1SGRNYQYwUFI/Sa2BHwCToyeQdCjwADDOzNYmMBbnnHMxSFhSMLMy4DLgdWAh8JyZzZd0i6Rx4WR3Au2A5yXNkTS5muKcc841gkQ2H2FmU4Apld67Mer16EQu3znnXN0kNCk0ltLSUvLz8ykpKUl2KC1Oeno6vXr1Ii0tLdmhOOcaQYtICvn5+bRv356+ffsiVXXRk6sPM2PDhg3k5+fTr1+/ZIfjnGsELaLvo5KSErp06eIJIc4k0aVLF6+BObcPaRFJAfCEkCC+Xp3bt7SYpOCcc67hPCnESWpqKiNGjCAnJ4fvfe97bN++vfaZovzlL3+p8zwAN954I2+99Vad56vKqFGjmDlzZlzKcs41T54U4iQjI4M5c+aQl5dH69atuf/++/cYb2aUl5dXO39NSWHXrl3VznfLLbcwerRf2euci48WcfVRtJtfnc+Cgi1xLXNIjyx+d+bQmKc//vjjmTt3LsuWLWPs2LGceOKJfPTRR7zyyissWrSI3/3ud+zYsYMBAwbwyCOP8PDDD1NQUMCJJ55IdnY206ZNo127dlx99dW8/vrr3H333bzzzju8+uqrFBcXc8wxx/DAAw8giQsvvJAzzjiDc845h759+/LjH/+YV199ldLSUp5//nkGDRrEtm3buPzyy5k3bx5lZWXcdNNNjB8/nuLiYiZMmMCCBQsYPHgwxcXFcV1vzrnmx2sKcVZWVsbUqVMZNmwYAIsWLeKCCy7gs88+o23bttx666289dZbzJ49m9zcXP70pz9xxRVX0KNHD6ZNm8a0adMA2LZtGzk5OXzyySccd9xxXHbZZcyYMYO8vDyKi4t57bXXqlx+dnY2s2fP5pJLLuGuu+4C4LbbbuOkk05ixowZTJs2jWuvvZZt27Zx3333kZmZydy5c7n++uuZNWtW46wk51yT1eJqCnU5oo+n4uJiRowYAQQ1hYsuuoiCggL69OnDyJEjAfj4449ZsGABxx57LAA7d+7k6KOPrrK81NRUzj777MjwtGnTuOOOO9i+fTsbN25k6NChnHnmmXvN993vfheAww8/nJdeegmAN954g8mTJ0eSRElJCcuXL2f69OlcccUVAAwfPpzhw4fHY1U455qxFpcUkqXinEJlbdu2jbw2M8aMGcMzzzxTa3np6emkpqYCwU780ksvZebMmfTu3Zubbrqp2nsH2rRpAwRJpaysLLLcF198kYMPPniv6f2SU+dcNG8+akQjR47kgw8+YMmSJQBs376dL7/8EoD27dtTVFRU5XwVCSA7O5utW7fywgsv1Gm53/72t7nnnnswCx5n8dlnnwFwwgkn8NRTTwGQl5fH3Llz6/6hnHMtiieFRtS1a1ceffRRzjvvPIYPH87IkSP54osvAJg4cWLkpHRlHTt25OKLL2bYsGGcddZZHHHEEXVa7g033EBpaSnDhw8nJyeHG264AYBLLrmErVu3Mnz4cO644w6OPPLIhn9I51yzpoqjx+YiNzfXKl9Lv3DhQgYPHpykiFo+X7/ONX+SZplZbm3TeU3BOedchCcF55xzEZ4UnHPORXhScM45F+FJwTnnXIQnBeeccxGeFOLotttuY+jQoQwfPpwRI0bwySefNKi8zZs387e//a3W6bzLa+dcvHhSiJOPPvqI1157jdmzZzN37lzeeustevfuXet8FV1RVCXWpOCcc/HS8vo+mnodrJ4X3zK7DYOxt9c4yapVq8jOzo70PZSdnQ3AjBkzuPLKK9m2bRtt2rTh7bff5sUXX+Rf//oXJSUlbNu2jcmTJzN+/Hg2bdpEaWkpt956K+PHj+e6667jq6++YsSIEYwZM4Y777yTO+64gyeeeIKUlBTGjh3L7bcHcT3//PNceumlbN68mYceeojjjz8+vuvAObdPaHlJIUlOOeUUbrnlFg466CBGjx7Nueeey9FHH825557Ls88+yxFHHMGWLVvIyMgAgprF3Llz6dy5M2VlZbz88stkZWWxfv16Ro4cybhx47j99tvJy8uLdLQ3depUXnnlFT755BMyMzPZuHFjZPllZWV8+umnTJkyhZtvvjluT2Nzzu1bWl5SqOWIPlHatWvHrFmzeO+995g2bRrnnnsu119/Pd27d4/0VZSVlRWZfsyYMXTu3BkIejH97W9/y/Tp00lJSWHlypWsWbNmr2W89dZbTJgwgczMTIDI/LBnl9nLli1L1Md0zrVwLS8pJFFqaiqjRo1i1KhRDBs2jHvvvbfarqmju9R+6qmnWLduHbNmzSItLY2+fftW2TW2mVVbXlVdZjvnXF35ieY4WbRoEYsXL44Mz5kzh8GDB1NQUMCMGTMAKCoqqnKHXVhYyH777UdaWhrTpk3jm2++AfbuTvuUU07h4YcfjjzLObr5yDnn4sFrCnGydetWLr/8cjZv3kyrVq048MADmTRpEhMmTODyyy+nuLiYjIyMKtv6zz//fM4880xyc3MZMWIEgwYNAqBLly4ce+yx5OTkMHbsWO68807mzJlDbm4urVu35rTTTuMPf/hDY39U51wL5l1nu1r5+nWu+fOus51zztWZJwXnnHMRLSYpNLdmsObC16tz+5YWkRTS09PZsGGD78DizMzYsGED6enpyQ7FOddIWsTVR7169SI/P59169YlO5QWJz09nV69eiU7DOdcI2kRSSEtLY1+/folOwznnGv2Etp8JOlUSYskLZF0XRXj20h6Nhz/iaS+iYzHOedczRKWFCSlAvcCY4EhwHmShlSa7CJgk5kdCPwZ+GOi4nHOOVe7RNYUjgSWmNlSM9sJ/AMYX2ma8cBj4esXgJNVXec+zjnnEi6R5xR6AiuihvOBo6qbxszKJBUCXYD10RNJmghMDAe3SlpUz5iyK5cdJ15u84o1UeU2p1ibW7nNKdamWm6fWCZKZFKo6oi/8jWjsUyDmU0CJjU4IGlmLLd5e7lNo8zmVm5zirW5lducYm2O5UZLZPNRPhD9PMpeQEF100hqBXQAvOtP55xLkkQmhRnAQEn9JLUGfgBMrjTNZODH4etzgHfM70BzzrmkSVjzUXiO4DLgdSAVeNjM5ku6BZhpZpOBh4AnJC0hqCH8IFHxhBrcBOXlNmqZza3c5hRrcyu3OcXaHMuNaHZdZzvnnEucFtH3kXPOufjwpOCccy5in0gKkh6WtFZSXpzL7S1pmqSFkuZLujIOZaZL+lTS52GZN8cj1qjyUyV9Jum1OJa5TNI8SXMkzax9jpjL7SjpBUlfhOv46AaWd3AYY8XfFklXxSnWX4TfV56kZyTFpWtZSVeGZc5vSKxV/QYkdZb0pqTF4f9OcSjze2Gs5ZLqdelkNeXeGW4HcyW9LKljnMr9fVjmHElvSOoRj3Kjxl0jySRlxyHWmyStjNp+T6trrDExsxb/B5wAHAbkxbnc7sBh4ev2wJfAkAaWKaBd+DoN+AQYGceYrwaeBl6LY5nLgOwEfG+PAT8NX7cGOsax7FRgNdAnDmX1BL4GMsLh54AL41BuDpAHZBJcFPIWMLCeZe31GwDuAK4LX18H/DEOZQ4GDgbeBXLjGOspQKvw9R/rGmsN5WZFvb4CuD8e5Ybv9ya40Oabuv4+qon1JuCahm5Xtf3tEzUFM5tOAu5/MLNVZjY7fF0ELCTYQTSkTDOzreFgWvgXl6sBJPUCTgcejEd5iSQpi+CH8RCAme00s81xXMTJwFdm9k2cymsFZIT322Sy9z059TEY+NjMtptZGfAf4Dv1Kaia30B0NzOPAWc1tEwzW2hm9e1xoKZy3wjXAcDHBPc9xaPcLVGDbanHb62G/cufgV/FucyE2yeSQmMIe3g9lODIvqFlpUqaA6wF3jSzBpcZ+gvBRloep/IqGPCGpFlhlyTx0B9YBzwSNnc9KKltnMqG4PLnZ+JRkJmtBO4ClgOrgEIzeyMORecBJ0jqIikTOI09bwhtqP3NbBUEBzjAfnEsO5F+AkyNV2GSbpO0AjgfuDFOZY4DVprZ5/EoL8plYXPXw3Vt7ouVJ4U4kNQOeBG4qtKRR72Y2S4zG0FwNHSkpJw4xHgGsNbMZjW0rCoca2aHEfSI+9+STohDma0Iqs/3mdmhwDaCJo4GC2+mHAc8H6fyOhEcdfcDegBtJf2woeWa2UKCppI3gX8DnwNlNc7Uwkm6nmAdPBWvMs3sejPrHZZ5WUPLCxP49cQpwUS5DxgAjCA4+Lg7zuUDnhQaTFIaQUJ4ysxeimfZYXPJu8CpcSjuWGCcpGUEPdaeJOnJOJSLmRWE/9cCLxP0kNtQ+UB+VC3pBYIkEQ9jgdlmtiZO5Y0GvjazdWZWCrwEHBOPgs3sITM7zMxOIGhOWByPckNrJHUHCP+vjWPZcSfpx8AZwPkWNrLH2dPA2XEoZwDBAcLn4e+tFzBbUreGFGpma8IDxnLg78Tnd7YXTwoNIEkEbd4LzexPcSqza8WVFZIyCHY4XzS0XDP7jZn1MrO+BE0n75hZg49mJbWV1L7iNcEJwQZf5WVmq4EVkg4O3zoZWNDQckPnEaemo9ByYKSkzHCbOJng/FKDSdov/H8A8F3iG3d0NzM/Bv4Zx7LjStKpwK+BcWa2PY7lDowaHEd8fmvzzGw/M+sb/t7yCS5IWd2QcisSeOg7xOF3VqVEn8luCn8EP6RVQCnBF3RRnMo9jqA9fS4wJ/w7rYFlDgc+C8vMA25MwPoYRZyuPiJo+/88/JsPXB/HOEcAM8N18QrQKQ5lZgIbgA5xXqc3E+xQ8oAngDZxKvc9gmT4OXByA8rZ6zdA0E392wS1j7eBznEo8zvh6x3AGuD1OMW6hKCb/YrfWX2uEqqq3BfD72wu8CrQMx7lVhq/jLpffVRVrE8A88JYJwPd47kNV/x5NxfOOecivPnIOedchCcF55xzEZ4UnHPORXhScM45F+FJwTnnXIQnBddshN09VPQQubpSj5GtYyzjkah7H6qb5r8lnR+nmN+XtCgqzmfjUW5U+fn16THUuer4JamuWZJ0E7DVzO6q9L4Itut49+9UL5LeBy4zszkJKj8fyLH4dhbo9mFeU3DNnqQDw2cO3A/MBrpLmiRpZti3/41R074vaYSkVpI2S7pdwbMrPoq6e/hWhc8uCKe/XcEzLhZJOiZ8v62kF8N5nwmXNaIOMT8p6T5J70n6UtLY8P0MSY8peD7F7Ip+pMJ4/xx+zrmSLo0q7qqw08C5kg4Kpz8pjG1OWE48OxN0LZgnBddSDAEeMrNDLei19DozywUOAcZIGlLFPB2A/5jZIcBHBL1vVkVmdiRwLbs7ObscWB3OeztBD7nVeTaq+ej2qPd7A98CzgQmSWpD0Kf/TjMbBvwIeCJsGruEoLO9Q8xsOEH/VRXWWNBp4IMEz8sgjHWiBR0rngCU1BCfcxGeFFxL8ZWZzYgaPk/SbIKaw2CCpFFZsZlVdME8C+hbTdkvVTHNcYQ7Zgu6R55fQ2znmtmI8C+6p9fnzKzcgucPrAAGhuU+EZY7n+C5DAcS9IF1v5ntCsdF97VfVXwfAH+RdDnBg2R21RCfcxGeFFxLsa3iRdjJ2ZXASeFR9b+Bqh6PuTPq9S6C7rqrsqOKadSgaAOVT+hZDeWqiukr7BWfmd0K/AxoB8yo1PGbc9XypOBaoiygCNgS9iz57QQs433g+wCShlF1TaQ231PgIIKmpMXAdIKHvSBpMMEjX5cAbwCXSEoNx3WuqWBJA8xsrpn9D0EHizVeceVcheqOjJxrzmYT9CyaBywlaEqJt3uAxyXNDZeXBxRWM+2zkorD12vMrCJJLSFIAvsRtP/vlHQP8ICkeQQ9ZF4Qvv8AQfPSXEllBA9cub+G+K6RdDzBU/bmEiQV52rll6Q6Vw8KnsXcysxKwqaZN4CBtvs5wrXN/yTwgpm9ksg4nasrryk4Vz/tgLfD5CDgZ7EmBOeaMq8pOOeci/ATzc455yI8KTjnnIvwpOCccy7Ck4JzzrkITwrOOeci/j+bd6moKS6EwQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the training curves of validation accuracy vs. number of training epochs for the transfer learning method and the model trained from scratch\n",
    "ohist = []\n",
    "shist = []\n",
    "\n",
    "ohist = [h.cpu().numpy() for h in hist]\n",
    "shist = [h.cpu().numpy() for h in scratch_hist]\n",
    "\n",
    "plt.title(\"Validation Accuracy vs. Number of Training Epochs\")\n",
    "plt.xlabel(\"Training Epochs\")\n",
    "plt.ylabel(\"Validation Accuracy\")\n",
    "plt.plot(range(1, num_epochs+1), ohist, label=\"Pretrained\")\n",
    "plt.plot(range(1, num_epochs+1), shist, label=\"Scratch\")\n",
    "plt.ylim((0, 1.))\n",
    "plt.xticks(np.arange(1, num_epochs+1, 1.0))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final thoughts\n",
    "# notice that feature extracting takes less time because in the backward pass we do not have to calculate most of the gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
