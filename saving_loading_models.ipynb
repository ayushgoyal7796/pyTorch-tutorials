{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving and Loading models\n",
    "# There are three core functions for saving and loading models:\n",
    "    # torch.save: saves a serialized object to disk. It uses pickle utility for serialization. Models, tensors, and dictionaries of all kinds of objects can be saved using this function.\n",
    "    # torch.load: uses pickle's unpickling facilities to deserialize pickled object files to memory. This function also facilitates the device to load the data into.\n",
    "    # torch.nn.Module.load_state_dict: Loads a model's parameter dictionary using a deserialized state_dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is a state_dict ?\n",
    "# Learnable parameters (weights and biases) of a torch.nn.Module model is contained in the model's parameters(accessed with model.parameters()).\n",
    "# A state_dict is simply a python dictionary object that maps each layer to its parameter tensor.\n",
    "# Only layers with learnable parameters (convolutional layers, linear layers, etc.) have entries in the model's state dict.\n",
    "# Optimizer objects (torch.optim) also have a state_dict, which contains information about optimizer's state, as well as the hyperparameters used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's state dict:\n",
      "conv1.weight \t torch.Size([6, 3, 5, 5])\n",
      "conv1.bias \t torch.Size([6])\n",
      "conv2.weight \t torch.Size([16, 6, 5, 5])\n",
      "conv2.bias \t torch.Size([16])\n",
      "fc1.weight \t torch.Size([120, 400])\n",
      "fc1.bias \t torch.Size([120])\n",
      "fc2.weight \t torch.Size([84, 120])\n",
      "fc2.bias \t torch.Size([84])\n",
      "fc3.weight \t torch.Size([10, 84])\n",
      "fc3.bias \t torch.Size([10])\n",
      "Optimizer's state dict:\n",
      "state \t {}\n",
      "param_groups \t [{'lr': 0.01, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'params': [1902700522016, 1902700522160, 1902700522232, 1902700522304, 1902700522376, 1902700522448, 1902700522520, 1902700522592, 1902700522664, 1902700522736]}]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define model\n",
    "class TheModelClass(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(TheModelClass, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2,2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16*5*5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16*5*5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "# Initialize the model\n",
    "model = TheModelClass()\n",
    "\n",
    "# Initialize the optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# Print model's state_dict\n",
    "print(\"Model's state dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, '\\t', model.state_dict()[param_tensor].size())\n",
    "\n",
    "# Print optimizer's state_dict\n",
    "print(\"Optimizer's state dict:\")\n",
    "for var_name in optimizer.state_dict():\n",
    "    print(var_name, '\\t', optimizer.state_dict()[var_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving and loading models for inference\n",
    "# save/load state_dict (Recommended)\n",
    "\n",
    "# save:\n",
    "    # torch.save(model.state_dict, PATH)\n",
    "\n",
    "# load:\n",
    "    # model = TheModelClass(*args, **kwargs)\n",
    "    # model.load_state_dict(torch.load(PATH))\n",
    "    # model.eval()\n",
    "\n",
    "# when saving a model for inferernce, it is only necessary to save the trained model’s learned parameters.\n",
    "# Saving the model’s state_dict with the torch.save() function will give you the most flexibility for restoring the model later, which is why it is the recommended method for saving models.\n",
    "# A common PyTorch convention is to save models using either a .pt or .pth file extension.\n",
    "\n",
    "# remember we must call model.eval() to set dropout and normalization layers to evaluation mode before running inference. Failing to do this will yield inconsistent inference results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note:\n",
    "    # the load_state_dict() function takes a dictionary object, not a path to a saved object.\n",
    "    # this means that you must deserialize the saved state_dict before you pass it to the load_state_dict() function.\n",
    "    # we cannot use model.load_state_dict(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save/load entire model\n",
    "\n",
    "# save:\n",
    "    # torch.save(model, PATH)\n",
    "    \n",
    "# load:\n",
    "    # Model class must be defined somewhere\n",
    "    # model = torch.load(PATH)\n",
    "    # model.eval()\n",
    "    \n",
    "# saving this way will save the entire module using Python's pickle module.\n",
    "# disadvantage: the serialized data is bound to the specific classes and the exact directory structure used when the model is saved.\n",
    "# The reason for this is because pickle does not save the model class itself.\n",
    "# Rather, it saves a path to the file containing the class, which is used during load time.\n",
    "# Because of this, your code can break in various ways when used in other projects or after refactors.\n",
    "\n",
    "# A common PyTorch convention is to save models using either a .pt or .pth file extension.\n",
    "# Remember that you must call model.eval() to set dropout and batch normalization layers to evaluation mode before running inference.\n",
    "# Failing to do this will yield inconsistent inference results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving and loading a general checkpoint for inference and/or resuming training\n",
    "\n",
    "# save:\n",
    "    # torch.save({\n",
    "    #     'epoch': epoch,\n",
    "    #     'model_state_dict': model.state_dict(),\n",
    "    #     'loss': loss,\n",
    "    #     ...\n",
    "    # }, PATH)\n",
    "    \n",
    "# load:\n",
    "    # model = TheModelClass(*args, **kwargs)\n",
    "    # optimizer = TheOptimizerClass(*args, **kwargs)\n",
    "    #\n",
    "    # checkpoint = torch.load(PATH)\n",
    "    # model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    # optimizer.load_state_dict(checkpoint['optimizer_state_dict'])    \n",
    "    # epoch = checkpoint['epoch']\n",
    "    # loss = checkpoint['loss']\n",
    "    #\n",
    "    # model.eval()\n",
    "    #   - or -\n",
    "    # model.train()\n",
    "    \n",
    "# when saving a general checkpoint, to be used for either inference or resuming training you must save more than just the model's state_dict.\n",
    "# it is also important to save the optimizer's state_dict, as this contains buffers and parameters that is updated as the model trains.\n",
    "# we may also want to save the epoch we  left off on, the latest recorded training loss etc.\n",
    "\n",
    "# To save multiple components, organize them in a dictionary and use torch.save() to serialize the dictionary.\n",
    "# A common PyTorch convention is to save these checkpoints using the .tar file extension.\n",
    "# To load, first initialize the model and optimizer, then load the dictionary locally using torch.load().\n",
    "\n",
    "# Remember that you must call model.eval() to set dropout and batch normalization layers to evaluation mode before running inference.\n",
    "# Failing to do this will yield inconsistent inference results.\n",
    "# If you wish to resuming training, call model.train() to ensure these layers are in training mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving multiple models in one file\n",
    "# save:\n",
    "    # torch.save({\n",
    "    #             'modelA_state_dict': modelA.state_dict(),\n",
    "    #             'modelB_state_dict': modelB.state_dict(),\n",
    "    #             'optimizerA_state_dict': optimizerA.state_dict(),\n",
    "    #             'optimizerB_state_dict': optimizerB.state_dict(),\n",
    "    #             ...\n",
    "    #             }, PATH)\n",
    "    \n",
    "# load:\n",
    "    # modelA = TheModelAClass(*args, **kwargs)\n",
    "    # modelB = TheModelBClass(*args, **kwargs)\n",
    "    # optimizerA = TheOptimizerAClass(*args, **kwargs)\n",
    "    # optimizerB = TheOptimizerBClass(*args, **kwargs)\n",
    "    # \n",
    "    # checkpoint = torch.load(PATH)\n",
    "    # modelA.load_state_dict(checkpoint['modelA_state_dict'])\n",
    "    # modelB.load_state_dict(checkpoint['modelB_state_dict'])\n",
    "    # optimizerA.load_state_dict(checkpoint['optimizerA_state_dict'])\n",
    "    # optimizerB.load_state_dict(checkpoint['optimizerB_state_dict'])\n",
    "    # \n",
    "    # modelA.eval()\n",
    "    # modelB.eval()\n",
    "    # # - or -\n",
    "    # modelA.train()\n",
    "    # modelB.train()\n",
    "    \n",
    "# When saving a model comprised of multiple torch.nn.Modules, such as a GAN, a sequence-to-sequence model, or an ensemble of models, you follow the same approach as when you are saving a general checkpoint.\n",
    "# Save a dictionary of each model’s state_dict and corresponding optimizer. \n",
    "# A common PyTorch convention is to save these checkpoints using the .tar file extension.\n",
    "# To load the models, first initialize the models and optimizers, then load the dictionary locally using torch.load().\n",
    "# we can easily access the saved items by simply querrying the dictionary.\n",
    "# we must call model.eval() to set dropout and batch normalization layers to evaluation mode before running inference.\n",
    "# Failing to do this will yield inconsistent inference results.\n",
    "# if you want to resume training call model.train() to set layers to training mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warmstarting model using parameters from a different model\n",
    "# Save:\n",
    "    # torch.save(modelA.state_dict, PATH)\n",
    "# Load:\n",
    "    # modelB = TheModelClass(*args, **kwars)\n",
    "    # modelB.load_state_dict(torch.load(PATH), strict=False)\n",
    "\n",
    "# Partially loading a model or loading a partial model are common scenarios when transfer learning or training a new complex model.\n",
    "# Leveraging trained parameters, even if only a few are usable, will help to warmstart the training process and hopefully help our model converge much faster than training from scratch.\n",
    "# Whether you are loading from a partial state_dict, which is missing some keys, or loading a state_dict with more keys than the model that you are loading into, you can set the strict argument to False in the load_state_dict() function to ignore non-matching keys.\n",
    "\n",
    "# If you want to load parameters from one layer to another, but some keys do not match, simply change the name of the parameter keys in the state_dict that you are loading to match the keys in the model that you are loading into."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving and Loading models across devices\n",
    "\n",
    "# save on GPU, load on CPU\n",
    "# save:\n",
    "    # torch.save(model.state_dict(), PATH)\n",
    "# load:\n",
    "    # device = torch.device('cpu')\n",
    "    # model = TheModelClass(*args, **kwargs)\n",
    "    # model.load_state_dict(torch.load(PATH, map_location=device))\n",
    "# When loading a model on a CPU that was trained with a GPU, pass torch.device('cpu') to the map_location argument in the torch.load() function.\n",
    "\n",
    "# save on GPU, load on GPU\n",
    "# save:\n",
    "    # torch.save(model.state_dict(), PATH)\n",
    "# load:\n",
    "    # device = torch.device(\"cuda\")\n",
    "    # model = TheModelClass(*args, **kwargs)\n",
    "    # model.load_state_dict(torch.load(PATH))\n",
    "    # model.to(device)\n",
    "# Make sure to call input = input.to(device) on any input tensors that you feed to the model\n",
    "\n",
    "# save on CPU, load on GPU\n",
    "# save:\n",
    "    # torch.save(model.state_dict(), PATH)\n",
    "# load:\n",
    "    # device = torch.device(\"cuda\")\n",
    "    # model = TheModelClass(*args, **kwargs)\n",
    "    # model.load_state_dict(torch.load(PATH, map_location=\"cuda:0\"))  # Choose whatever GPU device number you want\n",
    "    # model.to(device)   # to convert the model’s parameter tensors to CUDA tensors. \n",
    "# Make sure to call input = input.to(device) on any input tensors that you feed to the model\n",
    "\n",
    "# saving torch.nn.DataParallel models\n",
    "# save: \n",
    "    # torch.save(model.module.state_dict(), PATH)\n",
    "# load:\n",
    "    # load to whatever device you want\n",
    "# torch.nn.DataParallel is a model wrapper that enables parallel GPU utilization.\n",
    "# To save a DataParallel model generically, save the model.module.state_dict().\n",
    "# This way, you have the flexibility to load the model any way you want to any device you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
